{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "#Used headers/agent as the request timed out and asking for agent. Using following code you can fake the agent.\n",
    "headers={'User-Agent':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:66.0) Gecko/20100101 Firefox/66.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target url\n",
    "url=\"https://www.swiggy.com\"\n",
    "\n",
    "# Content from swiggy homepage and saving all the city name listed there\n",
    "response_city = requests.get(url,headers=headers)\n",
    "content_city = response_city.content\n",
    "soup_city = BeautifulSoup(content_city,\"html.parser\")\n",
    "name_city =soup_city.find_all(\"div\",attrs={\"class\": \"_2Im4A\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the links of all city on swiggy website\n",
    "\n",
    "%%time\n",
    "url_city=[]\n",
    "for i in range(3,7):\n",
    "    for a in name_city[i].find_all('a', href=True):\n",
    "            url_city.append(a['href'])\n",
    "print(\"Total City: \",len(url_city))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swiggy has listed restaurants in two ways.\n",
    "First, if the city is small it has listed all the restaurant as it is.\n",
    "Second, if the city is big then it is divided into region wise and all the restaurant are listed according to there region.\n",
    "So following code will go to each city page and check if has been divided into region or not. If yes, crawler will store links of all region otherwise it will store links of all restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all the links of region where city is divided into region\n",
    "# Saving all the links of the restaurants in a city\n",
    "%%time\n",
    "url_region=[]\n",
    "url_rest1=[]\n",
    "for i in range(len(url_city)):\n",
    "    try:\n",
    "        response_region = requests.get(url+url_city[i],headers=headers)\n",
    "        time.sleep(5)\n",
    "        if response_region.status_code==200:\n",
    "            content_region = response_region.content\n",
    "            soup_region = BeautifulSoup(content_region,\"html.parser\")\n",
    "            name_region =soup_region.find_all(\"div\",attrs={\"class\": \"zIQo_\"})\n",
    "            if not name_region:\n",
    "                page= soup_region.find_all(\"div\",attrs={\"class\": \"_2OmLw\"})\n",
    "                if not page:\n",
    "                    pass\n",
    "                else:            \n",
    "                    u=[]\n",
    "                    for a in page[0].find_all('a', href=True):\n",
    "                        if(a['href']=='#'):\n",
    "                            u.append(url+url_city[i])\n",
    "                        else:\n",
    "                            u.append(url+a['href'])\n",
    "                    for j in range(len(u)):\n",
    "                        response = requests.get(u[j],headers=headers)\n",
    "                        content = response.content\n",
    "                        soup = BeautifulSoup(content,\"html.parser\")\n",
    "                        link=soup.find_all(\"div\",attrs={\"class\": \"_3XX_A\"})\n",
    "                        for i in range(len(link)):\n",
    "                            for a in link[i].find_all('a', href=True):\n",
    "                                    url_rest1.append(a['href'])\n",
    "            else:\n",
    "                for l in range(len(name_region)):\n",
    "                    for a in name_region[l].find_all('a', href=True):\n",
    "                        url_region.append(a['href'])\n",
    "    except Exception as ex:\n",
    "        print(str(ex))\n",
    "print(\"Total Area: \", len(url_region))\n",
    "print(len(url_rest1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(url_rest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all the links of the resturant in a city\n",
    "\n",
    "%%time\n",
    "url_rest2=[]\n",
    "for i in range(len(url_region)):\n",
    "    try:\n",
    "        response_page = requests.get(url+url_region[i],headers=headers)\n",
    "        time.sleep(5)\n",
    "        if response_pag e.status_code==200:\n",
    "            content_page = response_page.content\n",
    "            soup_page = BeautifulSoup(content_page,\"html.parser\")\n",
    "            page= soup_page.find_all(\"div\",attrs={\"class\": \"_1MNQO\"})\n",
    "            if page:\n",
    "                u=[]\n",
    "                for a in page[0].find_all('a', href=True):\n",
    "                    u.append(a['href'])\n",
    "                for j in range(len(u)):\n",
    "                    response = requests.get(url+u[j],headers=headers)\n",
    "                    content = response.content\n",
    "                    soup = BeautifulSoup(content,\"html.parser\")\n",
    "                    data = json.loads(soup.find('script', type='application/ld+json').text)\n",
    "                    dic=data['itemListElement']\n",
    "                    for k in range(len(dic)):\n",
    "                        a=dic[k]['url']\n",
    "                        url_rest2.append(a)\n",
    "    except Exception as ex:\n",
    "        print(str(ex))\n",
    "print(len(url_rest2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total urls\n",
    "url_rest=url_rest1+url_rest2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting urls into dataframe\n",
    "links=pd.DataFrame()\n",
    "links['links']=url_rest\n",
    "links.to_csv(\"links.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
